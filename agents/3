import torch
import numpy as np
from importlib import import_module
from .default import NormalNN
from .default import *
from .regularization import SI, L2, EWC, MAS
from dataloaders.wrapper import Storage
import ipdb

from utils.metric import accuracy, AverageMeter, Timer
# from SubLayers import MultiHeadAttentionMemory
from transformer.SubLayers import MultiHeadAttentionMemory

class maxPoolFeedForward(nn.Module):
    ''' A two-feed-forward-layer module '''

    def __init__(self, d_in, d_hid, dropout=0.1):
        super().__init__()
        self.w_1 = nn.Linear(d_in, d_hid) # position-wise
        maxpool1d = nn.MaxPool1d(mp, stride=mp)
        self.w_2 = nn.Linear(d_hid//mp, d_hid//mp) # position-wise
        self.layer_norm = nn.LayerNorm(d_in, eps=1e-6)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):

        residual = x

        x = self.w_2(F.relu(self.w_1(x)))
        # x = self.dropout(x)
        mu = 0.1
        x += mu * residual

        x = self.layer_norm(x)

        return x
    
class OPT:
    def __init__(self, n_image_pixels):
        self.n_head = 8
        self.d_model = n_image_pixels
        self.d_k = 64
        self.d_v = 64
        self.max_pool_size = 2
        self.dropout = True

class MHA_pool(nn.Module):
    def __init__(self, n_image_pixels): 
        super().__init__()
        opt = OPT(n_image_pixels=n_image_pixels)  ### Parameter Class
        
        ### Multi-head attention for Memory 
        self.mul_head_attn1 = MultiHeadAttentionMemory(opt.n_head, opt.d_model, opt.d_k, opt.d_v, dropout=opt.dropout).cuda()
        self.mul_head_attn2 = MultiHeadAttentionMemory(opt.n_head, opt.d_model, opt.d_k, opt.d_v, dropout=opt.dropout).cuda()
        self.mha_fc_pool =  maxPoolFeedForward(opt.d_model, opt.d_model, opt.max_pool_size)
    
    def forward(self, M):
        M,     attn1 = self.mul_head_attn1(M, M, M)
        M_out, attn2 = self.mul_head_attn2(M, M, M)
        return M_out, attn2

    
class MHA(nn.Module):
    def __init__(self, n_image_pixels): 
        super().__init__()
        opt = OPT(n_image_pixels=n_image_pixels)  ### Parameter Class
        
        ### Multi-head attention for Memory 
        self.mul_head_attn1 = MultiHeadAttentionMemory(opt.n_head, opt.d_model, opt.d_k, opt.d_v, dropout=opt.dropout).cuda()
        self.mul_head_attn2 = MultiHeadAttentionMemory(opt.n_head, opt.d_model, opt.d_k, opt.d_v, dropout=opt.dropout).cuda()
        # self.mha_fc_pool =  maxPoolFeedForward(opt.d_model, opt.d_model, opt.max_pool_size)
    
    def forward(self, M):
        M,     attn1 = self.mul_head_attn1(M, M, M)
        M_out, attn2 = self.mul_head_attn2(M, M, M)
        return M_out, attn2

class Memory(Storage):
    def reduce(self, m):
        self.storage = self.storage[:m]


class Naive_Rehearsal(NormalNN):

    def __init__(self, agent_config):
        super(Naive_Rehearsal, self).__init__(agent_config)
        self.task_count = 0
        self.memory_size = 1000 ### Default Memory Size
        self.task_memory = {}

    def learn_batch(self, train_loader, val_loader=None):
        # 1.Combine training set
        dataset_list = []

        ### self.task_memory is dict()
        for storage in self.task_memory.values():
            ### strorgae -> list and each element is ([1, 32, 32], int, str)
            dataset_list.append(storage)

        dll = len(dataset_list) 
        ### self.memory_size affects training data here.

        ### We assume memory size is full 
        dataset_list *= max(len(train_loader.dataset)//self.memory_size,1)  # Let old data: new data = 1:1 ### Not exactly 1:1

        dataset_list.append(train_loader.dataset)

        ### Wrap mixed dataset (dataset_list).
            # pdb.set_trace()
        dataset = torch.utils.data.ConcatDataset(dataset_list)

        new_train_loader = torch.utils.data.DataLoader(dataset,
                                                       batch_size=train_loader.batch_size,
                                                       shuffle=True,
                                                       num_workers=train_loader.num_workers)

        # 2.Update model as normal
        super(Naive_Rehearsal, self).learn_batch(new_train_loader, val_loader)

        # 3.Randomly decide the images to stay in the memory
        self.task_count += 1
       
        # (a) Decide the number of samples for being saved

        ### Let memory size M = self.memory_size 
        ### the first task, M, the second M/2, the third M/3... Memory Diverges - cf) \Sigma_{1}^{\infinity} 1/n
        num_sample_per_task = self.memory_size // self.task_count
        num_sample_per_task = min(len(train_loader.dataset),num_sample_per_task)
       
       
        # (b) Reduce current exemplar set to reserve the space for the new dataset
        for storage in self.task_memory.values():
            storage.reduce(num_sample_per_task)
        
        # (c) Randomly choose some samples from new task and save them to the memory
        self.task_memory[self.task_count] = Memory()  # Initialize the memory slot
        randind = torch.randperm(len(train_loader.dataset))[:num_sample_per_task]  # randomly sample some data
        for ind in randind:  # save it to the memory
            self.task_memory[self.task_count].append(train_loader.dataset[ind])


class Fed_Memory_Rehearsal(NormalNN):
    def __init__(self, agent_config):
        super(Fed_Memory_Rehearsal, self).__init__(agent_config)
        self.task_count = 0
        self.memory_size = 4000 ### Default Memory Size
        self.task_memory = {}

        ### +--------------------------------
        ### | Train data specifications
        ### +--------------------------------
        self.ch = 1
        self.n_image_pixels = 32*32
        self.task_numbers = 100
        self.n_max_label = 100
        self.ls = 1000


        ### +--------------------------------
        ### | Model Specification
        ### +--------------------------------
        self.MHA_model = MHA(self.n_image_pixels)
        MHA_params = self.MHA_model.parameters()
        self.mha_optimizer = torch.optim.Adam(MHA_params, lr=0.0001)
        # self.mha_optimizer = torch.optim.SGD(MHA_params, lr=0.0001)
        self.neural_task_memory = {str(x+1): { y: {z: [] for z in range(self.ls)} for y in range(self.n_max_label)} for x in range(self.task_numbers)}
        # self.neural_task_memory = {str(x+1): [] for x in range(self.task_numbers)}

    def reqGradToggle(self, _bool):
        for layer in self.model.parameters():
            layer.requires_grad = _bool
        self.model.train(_bool)
    
    # @staticmethod
    def aggregatedClassLoss(self, preds, target):
        if isinstance(self.valid_out_dim, int):  
            pred = preds['All'][:,:self.valid_out_dim]
        else:
            if type(preds) == type(dict()) and 'All' in preds.keys(): # Single-headed model
                pred = preds['All']
            else:
                pred = preds
        loss = self.criterion_fn(pred, target)
        return loss 

    
    def getCompressedMemory(self, new_train_loader):
        max_n_train_mha = 1
        for epoch_idx in range(max_n_train_mha):
            print("*Compressed Mem Epoch: {}/{}".format(epoch_idx+1, max_n_train_mha) )
            batch_acc = AverageMeter()
            for i, (input, target, task) in enumerate(new_train_loader):
                # print("task : ", task[0], flush=True)
                if i == len(new_train_loader)-1:
                    break
                if self.gpu:
                    with torch.no_grad():
                        input = input.cuda()
                        target = target.cuda()
                M = input
                n_image_pixels = M.shape[2] * M.shape[3]
                M = M.view(-1, self.ch, n_image_pixels)            

                # max_n_train_mha = 1000000
                labels = sorted(list(set(target.cpu().numpy())))

                self.MHA_model.train()
                acc = AverageMeter()
                c_loss = 0
                out_box, label_box = [], []
                for idx, label in enumerate(labels):
                    acc_label = AverageMeter()
                    task_idx = task[idx]
                    # self.neural_task_memory[task_idx][label] = \
                        # input[target==label, :].view(-1, self.ch, self.n_image_pixels)
                    this_target = target[target == label]
                    
                    M_before_integ = input[target == label, :].view(-1, self.ch, self.n_image_pixels)
                    M_in = M_before_integ.unsqueeze(dim=0)  ### Batch --> input_length
                    
                    # if task_idx == '1' and epoch_idx == 0:
                    if epoch_idx == 0:
                        M_before_integ = input[target == label, :].view(-1, self.ch, self.n_image_pixels)
                        M_in = M_before_integ.unsqueeze(dim=0)  ### Batch --> input_length
                    else:
                        M_in = self.neural_task_memory[task_idx][label][i].cuda()
                        M_before_integ = M_in

                    self.mha_optimizer.zero_grad()
                    try:
                        # print("M_in dim:", M_in.shape)
                        # M, attention = self.mul_head_attn(M_in, M_in, M_in)
                        M, attention = self.MHA_model(M_in)
                    except:
                        ipdb.set_trace()
                    # M = M[0,:,:]
                    # else:
                        # M = self.neural_task_memory[task_idx][label]
                    try:
                        # ipdb.set_trace()
                        # M = M[0]
                        M_imaged = M.view(M_before_integ.shape)
                        
                        # M = M.mean(dim=1)
                        # M = M[0][-1].unsqueeze(0)
                        # this_target = target[target == label][0].unsqueeze(0)
                        # M_imaged = M
                    except:
                        ipdb.set_trace()
                    try:
                        if len(M_imaged) == 3:
                            M_imaged = M_imaged.unsqueeze(0)

                        self.neural_task_memory[task_idx][label][i] = M_imaged.detach().cpu()
                        # print("Shape of M_imaged", M_imaged.shape)
                        
                    except:
                        ipdb.set_trace()

                    MHA_output = self.model.forward(M_imaged)
                    # out_box.append(MHA_output['All'])
                    # label_box.append(this_target)
                    # print("Label: {} ACC: {}".format(label, acc_label.avg))
                    # print("M:", M[0][0][:])
                    # batch_MHA_output = {'All' :torch.cat(out_box, dim=0) }
                    # batch_target = torch.cat(label_box, dim=0)
                    try:
                        # class_loss = self.aggregatedClassLoss(batch_MHA_output, batch_target)
                        class_loss = self.aggregatedClassLoss(MHA_output, this_target)
                    except:
                        ipdb.set_trace()
                    # class_loss.backward(retain_graph=True)
                    class_loss.backward()
                    self.mha_optimizer.step()
                    
                    acc = accumulate_acc(MHA_output, this_target, task, acc)
                    batch_acc = accumulate_acc(MHA_output, this_target, task, batch_acc)
                    # acc_label = accumulate_acc(batch_MHA_output, batch_target, task, acc_label)
                    c_loss += class_loss.detach().cpu().numpy()
                # print("Mini-Batch {}/{} ACC: {:2.4f} Loss: {:2.4f}".format(i+1, len(new_train_loader), acc.avg, c_loss))
                # batch_acc = accumulate_acc(MHA_output, this_target, task, batch_acc)
            print("Epoch Acc {}/{} ACC: {:2.4f} ".format(epoch_idx+1, max_n_train_mha, batch_acc.avg))
            batch_acc = accumulate_acc(MHA_output, this_target, task, batch_acc)
        return None
    
    def learn_batch(self, train_loader, val_loader=None):
        # 1.Combine training set
        dataset_list = []

        ### self.task_memory is dict()
        for storage in self.task_memory.values():

            ### strorgae -> list and each element is ([1, 32, 32], int, str)
            dataset_list.append(storage)
        dll = len(dataset_list) 
        ### self.memory_size affects training data here.

        ### We assume memory size is full 
        dataset_list *= max(len(train_loader.dataset)//self.memory_size,1)  # Let old data: new data = 1:1 ### Not exactly 1:1

        dataset_list.append(train_loader.dataset)

        ### Wrap mixed dataset (dataset_list).
            # pdb.set_trace()
        dataset = torch.utils.data.ConcatDataset(dataset_list)

        new_train_loader = torch.utils.data.DataLoader(dataset,
                                                       batch_size=train_loader.batch_size,
                                                       shuffle=False,
                                                       num_workers=train_loader.num_workers)
        # 2.Update model as normal
        # super(Fed_Memory_Rehearsal, self).learn_batch(new_train_loader, val_loader)
        self.memory_learn_batch(new_train_loader, val_loader)
            

        # 3.Randomly decide the images to stay in the memory
    
        self.task_count += 1
       
        # (a) Decide the number of samples for being saved

        ### Let memory size M = self.memory_size 
        ### the first task, M, the second M/2, the third M/3... Memory Diverges - cf) \Sigma_{1}^{\infinity} 1/n
        num_sample_per_task = self.memory_size // self.task_count
        num_sample_per_task = min(len(train_loader.dataset),num_sample_per_task)
       
        # (b) Reduce current exemplar set to reserve the space for the new dataset
        for storage in self.task_memory.values():
            storage.reduce(num_sample_per_task)
       
        if method == "Naive_Rehearsal":
        # (c) Randomly choose some samples from new task and save them to the memory
        self.task_memory[self.task_count] = Memory()  # Initialize the memory slot
        randind = torch.randperm(len(train_loader.dataset))[:num_sample_per_task]  # randomly sample some data
        for ind in randind:  # save it to the memory
            self.task_memory[self.task_count].append(train_loader.dataset[ind])
        elif method == "Fed_Memory_Rehearsal":
            self.reqGradToggle(False)  ### Freeze the model
            self.getCompressedMemory(new_train_loader)
            self.reqGradToggle(True)  ### Freeze the model
        else:
            raise ValueError('method name {} does not exist.'.format(method ) )
        # ipdb.set_trace()
    def org_learn_batch(self, train_loader, val_loader=None):
        # 1.Combine training set
        dataset_list = []

        ### self.task_memory is dict()
        for storage in self.task_memory.values():

            ### strorgae -> list and each element is ([1, 32, 32], int, str)
            dataset_list.append(storage)
        dll = len(dataset_list) 
        ### self.memory_size affects training data here.

        ### We assume memory size is full 
        dataset_list *= max(len(train_loader.dataset)//self.memory_size,1)  # Let old data: new data = 1:1 ### Not exactly 1:1

        dataset_list.append(train_loader.dataset)

        ### Wrap mixed dataset (dataset_list).
            # pdb.set_trace()
        dataset = torch.utils.data.ConcatDataset(dataset_list)

        new_train_loader = torch.utils.data.DataLoader(dataset,
                                                       batch_size=train_loader.batch_size,
                                                       shuffle=False,
                                                       num_workers=train_loader.num_workers)
        # 2.Update model as normal
        # super(Fed_Memory_Rehearsal, self).learn_batch(new_train_loader, val_loader)
        self.memory_learn_batch(new_train_loader, val_loader)
            
        self.reqGradToggle(False)  ### Freeze the model
        self.getCompressedMemory(new_train_loader)
        self.reqGradToggle(True)  ### Freeze the model

        # 3.Randomly decide the images to stay in the memory
    
        self.task_count += 1
       
        # (a) Decide the number of samples for being saved

        ### Let memory size M = self.memory_size 
        ### the first task, M, the second M/2, the third M/3... Memory Diverges - cf) \Sigma_{1}^{\infinity} 1/n
        num_sample_per_task = self.memory_size // self.task_count
        num_sample_per_task = min(len(train_loader.dataset),num_sample_per_task)
       
        # (b) Reduce current exemplar set to reserve the space for the new dataset
        for storage in self.task_memory.values():
            storage.reduce(num_sample_per_task)
        
        # (c) Randomly choose some samples from new task and save them to the memory
        self.task_memory[self.task_count] = Memory()  # Initialize the memory slot
        randind = torch.randperm(len(train_loader.dataset))[:num_sample_per_task]  # randomly sample some data
        for ind in randind:  # save it to the memory
            self.task_memory[self.task_count].append(train_loader.dataset[ind])
    
    def memory_learn_batch(self, train_loader, val_loader=None):
        # ipdb.set_trace()        
        if self.reset_optimizer:  # Reset optimizer before learning each task
            self.log('Optimizer is reset!')
            self.init_optimizer()

        for epoch in range(self.config['schedule'][-1]):
            data_timer = Timer()
            batch_timer = Timer()
            batch_time = AverageMeter()
            data_time = AverageMeter()
            losses = AverageMeter()
            acc = AverageMeter()

            # Config the model and optimizer
            self.log('Epoch:{0}'.format(epoch))
            self.model.train()
            self.scheduler.step(epoch)
            for param_group in self.optimizer.param_groups:
                self.log('LR:',param_group['lr'])

            # Learning with mini-batch
            data_timer.tic()
            batch_timer.tic()
            self.log('Itr\t\tTime\t\t  Data\t\t  Loss\t\tAcc')
            for i, (input, target, task) in enumerate(train_loader):

                data_time.update(data_timer.toc())  # measure data loading time

                if self.gpu:
                    input = input.cuda()
                    target = target.cuda()

                loss, output = self.update_model(input, target, task)
                input = input.detach()
                target = target.detach()

                # measure accuracy and record loss
                acc = accumulate_acc(output, target, task, acc)
                losses.update(loss, input.size(0))

                batch_time.update(batch_timer.toc())  # measure elapsed time
                data_timer.toc()

                if ((self.config['print_freq']>0) and (i % self.config['print_freq'] == 0)) or (i+1)==len(train_loader):
                    self.log('[{0}/{1}]\t'
                          '{batch_time.val:.4f} ({batch_time.avg:.4f})\t'
                          '{data_time.val:.4f} ({data_time.avg:.4f})\t'
                          '{loss.val:.3f} ({loss.avg:.3f})\t'
                          '{acc.val:.2f} ({acc.avg:.2f})'.format(
                        i, len(train_loader), batch_time=batch_time,
                        data_time=data_time, loss=losses, acc=acc))

            self.log(' * Train Acc {acc.avg:.3f}'.format(acc=acc))

            # Evaluate the performance of current task
            if val_loader != None:
                self.validation(val_loader)


class Naive_Rehearsal_SI(Naive_Rehearsal, SI):

    def __init__(self, agent_config):
        super(Naive_Rehearsal_SI, self).__init__(agent_config)


class Naive_Rehearsal_L2(Naive_Rehearsal, L2):

    def __init__(self, agent_config):
        super(Naive_Rehearsal_L2, self).__init__(agent_config)


class Naive_Rehearsal_EWC(Naive_Rehearsal, EWC):

    def __init__(self, agent_config):
        super(Naive_Rehearsal_EWC, self).__init__(agent_config)
        self.online_reg = True  # Online EWC


class Naive_Rehearsal_MAS(Naive_Rehearsal, MAS):

    def __init__(self, agent_config):
        super(Naive_Rehearsal_MAS, self).__init__(agent_config)


class GEM(Naive_Rehearsal):
    """
    @inproceedings{GradientEpisodicMemory,
        title={Gradient Episodic Memory for Continual Learning},
        author={Lopez-Paz, David and Ranzato, Marc'Aurelio},
        booktitle={NIPS},
        year={2017},
        url={https://arxiv.org/abs/1706.08840}
    }
    """

    def __init__(self, agent_config):
        super(GEM, self).__init__(agent_config)
        self.params = {n: p for n, p in self.model.named_parameters() if p.requires_grad}  # For convenience
        self.task_grads = {}
        self.quadprog = import_module('quadprog')
        self.task_mem_cache = {}

    def grad_to_vector(self):
        vec = []
        for n,p in self.params.items():
            if p.grad is not None:
                vec.append(p.grad.view(-1))
            else:
                # Part of the network might has no grad, fill zero for those terms
                vec.append(p.data.clone().fill_(0).view(-1))
        return torch.cat(vec)

    def vector_to_grad(self, vec):
        # Overwrite current param.grad by slicing the values in vec (flatten grad)
        pointer = 0
        for n, p in self.params.items():
            # The length of the parameter
            num_param = p.numel()
            if p.grad is not None:
                # Slice the vector, reshape it, and replace the old data of the grad
                p.grad.copy_(vec[pointer:pointer + num_param].view_as(p))
                # Part of the network might has no grad, ignore those terms
            # Increment the pointer
            pointer += num_param

    def project2cone2(self, gradient, memories):
        """
            Solves the GEM dual QP described in the paper given a proposed
            gradient "gradient", and a memory of task gradients "memories".
            Overwrites "gradient" with the final projected update.

            input:  gradient, p-vector
            input:  memories, (t * p)-vector
            output: x, p-vector

            Modified from: https://github.com/facebookresearch/GradientEpisodicMemory/blob/master/model/gem.py#L70
        """
        margin = self.config['reg_coef']
        memories_np = memories.cpu().contiguous().double().numpy()
        gradient_np = gradient.cpu().contiguous().view(-1).double().numpy()
        t = memories_np.shape[0]
        #print(memories_np.shape, gradient_np.shape)
        P = np.dot(memories_np, memories_np.transpose())
        P = 0.5 * (P + P.transpose())
        q = np.dot(memories_np, gradient_np) * -1
        G = np.eye(t)
        P = P + G * 0.001
        h = np.zeros(t) + margin
        v = self.quadprog.solve_qp(P, q, G, h)[0]
        x = np.dot(v, memories_np) + gradient_np
        new_grad = torch.Tensor(x).view(-1)
        if self.gpu:
            new_grad = new_grad.cuda()
        return new_grad

    def learn_batch(self, train_loader, val_loader=None):

        # 1.Update model as normal
        super(GEM, self).learn_batch(train_loader, val_loader)

        # 2.Randomly decide the images to stay in the memory
        self.task_count += 1
        # (a) Decide the number of samples for being saved
        num_sample_per_task = self.memory_size // self.task_count
        num_sample_per_task = min(len(train_loader.dataset),num_sample_per_task)
        # (b) Reduce current exemplar set to reserve the space for the new dataset
        for storage in self.task_memory.values():
            storage.reduce(num_sample_per_task)
        # (c) Randomly choose some samples from new task and save them to the memory
        self.task_memory[self.task_count] = Memory()  # Initialize the memory slot
        randind = torch.randperm(len(train_loader.dataset))[:num_sample_per_task]  # randomly sample some data
        for ind in randind:  # save it to the memory
            self.task_memory[self.task_count].append(train_loader.dataset[ind])
        # (d) Cache the data for faster processing
        for t, mem in self.task_memory.items():
            # Concatenate all data in each task
            mem_loader = torch.utils.data.DataLoader(mem,
                                                     batch_size=len(mem),
                                                     shuffle=False,
                                                     num_workers=2)
            assert len(mem_loader)==1,'The length of mem_loader should be 1'
            for i, (mem_input, mem_target, mem_task) in enumerate(mem_loader):
                if self.gpu:
                    mem_input = mem_input.cuda()
                    mem_target = mem_target.cuda()
            self.task_mem_cache[t] = {'data':mem_input,'target':mem_target,'task':mem_task}

    def update_model(self, inputs, targets, tasks):

        # compute gradient on previous tasks
        if self.task_count > 0:
            for t,mem in self.task_memory.items():
                self.zero_grad()
                # feed the data from memory and collect the gradients
                mem_out = self.forward(self.task_mem_cache[t]['data'])
                mem_loss = self.criterion(mem_out, self.task_mem_cache[t]['target'], self.task_mem_cache[t]['task'])
                mem_loss.backward()
                # Store the grads
                self.task_grads[t] = self.grad_to_vector()

        # now compute the grad on the current minibatch
        out = self.forward(inputs)
        loss = self.criterion(out, targets, tasks)
        self.optimizer.zero_grad()
        loss.backward()

        # check if gradient violates constraints
        if self.task_count > 0:
            current_grad_vec = self.grad_to_vector()
            mem_grad_vec = torch.stack(list(self.task_grads.values()))
            dotp = current_grad_vec * mem_grad_vec
            dotp = dotp.sum(dim=1)
            if (dotp < 0).sum() != 0:
                new_grad = self.project2cone2(current_grad_vec, mem_grad_vec)
                # copy gradients back
                self.vector_to_grad(new_grad)

        self.optimizer.step()
        return loss.detach(), out
